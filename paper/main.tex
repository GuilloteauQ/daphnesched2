\documentclass[conference,10pt,a4paper]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{hyperref}
\usepackage{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{cleveref}
\newtcbtheorem[]{lesson}{Observation}{colback=black!5,colframe=black!35,fonttitle=\bfseries}{th}

\newcommand{\ie}{\emph{i.e.,}}
\newcommand{\eg}{\emph{e.g.,}}
\newcommand{\ds}{\emph{DaphneSched}}

\newcommand{\fmc}[1]{{\color{magenta} FMC: #1}} % Florina Ciorba
\newcommand{\jk}[1]{{\color{orange} JK: #1}} % Jonas
\newcommand{\qg}[1]{{\color{blue} QG: #1}} % Quentin

%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage[
  datamodel=software,
  style=trad-abbrv,
  backend=biber
]{biblatex}
\addbibresource{references.bib}
\usepackage{software-biblatex}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

%\title{Effortless parallelization and distribution with Daphne}
% \title{Effortless Parallel and Distributed Execution with Daphne}
\title{Effortless Distributed Execution with Daphne}

\author{
\IEEEauthorblockN{Anonymous Authors}
\IEEEauthorblockA{Anonymous Affiliation\\
City, Country \\
}
}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
  TODO
\end{abstract}

\begin{IEEEkeywords}
TODO
\end{IEEEkeywords}

\section{Introduction}

\begin{itemize}
\item IDA pipelines
\item different paradigms, different hardware
\item etc.
\item take some text from the ispdc paper
\end{itemize}

Contributions:

\begin{itemize}
\item \textbf{Comparison between DaphneSched and popular scientific languages}
\item \textbf{Study the impact of scheduling on performance with the flexibility of DaphneSched}
\end{itemize}

\subsection{try}


When developing a new scientific application, researchers are faced with a dilema:
either develop quickly with scripting languages such a Python which might yield poor performance,
or invest more time and efforts into a implementation in fast compiled languages such as C++/Rust.
Moreover, if an appliction is initially written in Python, and is lacking performance, it will need to be fully rewritten in another language.
This issue is known as the ``Two languages problem''.
Julia \cite{bezanson2012julia} positioned itself as a solution combining both the ease of writing of a scripting language, while yielding high performance with a Just-in-Time compilation.


% However, the ``Two language problem'' still persist when the application needs to scale beyond a single thread or a single machine.
Some libraries, such as Numpy \cite{harris2020array}, are parallelizable and allow user to use the full computing power of a single machine without having to modify any of their application.
However, when needed to scale beyond a single machine, no language nor library allow to easily distribute the computation by requiring no modification of the user's code.
If users need to distribute their computations, they will need, at best, a partial rewrite of their application to integrate MPI for example.
Hence, the ``Two language problem'' persists.

In this paper, we present the extension of \ds\ \cite{eleliemy2023daphnesched} to support the distributed scheduling and execution of integrated data analysis pipelines.



\section{Background}

Daphne \cite{damme2022daphne}

Chapel \cite{callahan2004cascade}

StarPU \cite{augonnet2009starpu} 

HPX \cite{kaiser2014hpx}

Julia \cite{bezanson2012julia}

Numpy \cite{harris2020array}

R \cite{morandat2012evaluating}

LB4OMP \cite{korndorfer2021lb4omp}

\section{DaphneSched}

DaphneSched \cite{eleliemy2023daphnesched}

\qg{summary of the local runtime, queues, scheduling techniques, (victim selections), and then description of the MPI distributed runtime (+ mention of gRPC ?)}

\section{Importance of Scheduling}

\qg{in this section we show how the different option of daphnesched impact the performance. should this be before the section on comparison with different languages ?
It can also be presented as a platform for studying scheduling.
}

\subsection{Considered Benchmarks}

\qg{TODO: small descriptions with math equations if applicable, give use-cases of algos}

\paragraph{Connected Components}

\paragraph{Page Rank}

assigns an ``importance'' weight to each node of $G$ based on the number of edges pointing from and to.

\paragraph{k-core}

computes the subgraph of $G$ where all the nodes have at least a degree of $k$.
To do so, it iteratively removes the nodes of unsufficient degree.

\paragraph{Linear Regression}

\paragraph{k-means}

\paragraph{N-body Simulation}

simulates the gravitational interaction between $N$ bodies.


\paragraph{\qg{something from reto bsc thesis?}}

\subsection{Experiments}

\qg{on VEGA?, sciCORE?, miniHPC?}

\section{Comparison with popular scientific languages}

\begin{itemize}
    \item implement algorithms in an \textbf{idiomatic} way
    \item C++, Julia, Python, and DaphneDSL
\end{itemize}

\subsection{Comparison}

\begin{itemize}
\item number of lines
\item number of third party dependencies
\item distance between code and math equations
\item ``difficulty'' of implementation
\item \qg{disk size of binary + deps ?}
\end{itemize}

\paragraph{Third party dependencies}

One drawback of using general purpose programming languages for scientific computing is the need for third party dependencies.
These dependencies add complexity to the software environment \qg{more + repro?}.

The C++ implementations of the considered benchmarks depend on the Eigen library
Eigen \cite{guennebaud2010eigen} is a C++ library for linear algebra.
It supports operations on dense and spare matrices.
Eigen is parallelized via OpenMP.
Users can disable this parallelization at compile time, or set the number of threads via the \texttt{OMP\_NUM\_THREADS} environment variable.

The Python implementations depend on Numpy and Scipy.
Numpy \cite{harris2020array} and SciPy \cite{virtanen2020scipy} are the corner stone of scientific computing in Python.
The later depends on the former which is also parallelized via OpenMP.
Similarly to Eigen, users can set up \texttt{OMP\_NUM\_THREADS} to increase the number of threads.

The Julia implementations only required one external library to load the matrices of the MatrixMarket format.
All the computations use the standard Julia library. 

No external dependencies were needed for the DaphneDSL implementations.

\subsection{Considered Matrices}

The matrices have been optained on the \textit{SuiteSparse Matrix Collection} \cite{davis2011university}. 

\paragraph{\texttt{webbase-2001} \cite{BoVWFI, BRSLLP}}
which has been obtained from a 2001 crawl performed by the WebBase crawler.
The matrix represents the connections between 118 142 155 websites.



\subsection{Experiments}

\qg{on VEGA?, sciCORE?, miniHPC?}

\section{Conlusion and Perspectives}

\section*{Acknowledgments}

This research was funded, in whole or in part, by the European Union’s Horizon 2020 research and innovation programme under grant agreement No. 957407 as DAPHNE.
The authors have applied a CC-BY public copyright license to the present document and will be applied to all subsequent versions up to the Author Accepted Manuscript arising from this submission, in accordance with the grant’s open access conditions.

\printbibliography

\end{document}
