\documentclass[conference,10pt,a4paper]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.

\usepackage{hyperref}
\usepackage{tcolorbox}
\tcbuselibrary{theorems}
\usepackage{cleveref}
\newtcbtheorem[]{lesson}{Observation}{colback=black!5,colframe=black!35,fonttitle=\bfseries}{th}

\newcommand{\ie}{\emph{i.e.,}}
\newcommand{\eg}{\emph{e.g.,}}
\newcommand{\ds}{\emph{DaphneSched}}

\newcommand{\fmc}[1]{{\color{magenta} FMC: #1}} % Florina Ciorba
\newcommand{\jk}[1]{{\color{orange} JK: #1}} % Jonas
\newcommand{\qg}[1]{{\color{blue} QG: #1}} % Quentin

%\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\usepackage[
  datamodel=software,
  style=trad-abbrv,
  backend=biber
]{biblatex}
\addbibresource{references.bib}
\usepackage{software-biblatex}


\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

%\title{Effortless parallelization and distribution with Daphne}
% \title{Effortless Parallel and Distributed Execution with Daphne}
\title{Effortless Distributed Execution with Daphne}
% \title{Effortless Scalable Execution with Daphne}

\author{
\IEEEauthorblockN{Anonymous Authors}
\IEEEauthorblockA{Anonymous Affiliation\\
City, Country \\
}
}

\maketitle
\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
  TODO
\end{abstract}

\begin{IEEEkeywords}
TODO
\end{IEEEkeywords}

\section{Introduction}

\begin{itemize}
\item IDA pipelines
\item different paradigms, different hardware
\item etc.
\item take some text from the ispdc paper
\end{itemize}

Contributions:

\begin{itemize}
\item \textbf{Comparison between DaphneSched and popular scientific languages}
\item \textbf{Study the impact of scheduling on performance with the flexibility of DaphneSched}
\end{itemize}

\subsection{try}


When developing a new scientific application, researchers are faced with a dilema:
either develop quickly with scripting languages such a Python which might yield poor performance,
or invest more time and efforts into a implementation in fast compiled languages such as C++/Rust.
Moreover, if an appliction is initially written in Python, and is lacking performance, it will need to be fully rewritten in another language.
This issue is known as the ``Two languages problem''.
Julia \cite{bezanson2012julia} positioned itself as a solution combining both the ease of writing of a scripting language, while yielding high performance with a Just-in-Time compilation.


% However, the ``Two language problem'' still persist when the application needs to scale beyond a single thread or a single machine.
Some libraries, such as Numpy \cite{harris2020array}, are parallelizable and allow user to use the full computing power of a single machine without having to modify any of their application.
However, when needed to scale beyond a single machine, no language nor library allow to easily distribute the computation by requiring no modification of the user's code.
If users need to distribute their computations, they will need, at best, a partial rewrite of their application to integrate MPI for example.
Hence, the ``Two language problem'' persists.

In this paper, we present the extension of \ds\ \cite{eleliemy2023daphnesched} to support the distributed scheduling and execution of integrated data analysis pipelines.



\section{Background}

Daphne \cite{damme2022daphne} is a scalable infrastructure system designed for integrated data analysis pipelines, \ie\ the management and processing of high performance computation or machine learning data and \qg{scoring}.

Chapel \cite{callahan2004cascade}

StarPU \cite{augonnet2009starpu} is a runtime for parallelize and distribute computation on heterogenous machines.
Users annotate their code to give some hints to the runtime about the nature of the computation.

HPX \cite{kaiser2014hpx}

Julia \cite{bezanson2012julia}

Numpy \cite{harris2020array}

R \cite{morandat2012evaluating}

LB4OMP \cite{korndorfer2021lb4omp} is a extension of OpenMP with state-of-the-art scheduling techniques (\eg\ \qg{list some}).

\qg{LB4MPI ?}


\section{DaphneSched}

DaphneSched \cite{eleliemy2023daphnesched}

\qg{summary of the local runtime, queues, scheduling techniques, (victim selections), and then description of the MPI distributed runtime (+ mention of gRPC ?)}

\subsection{Terminology}

From the point of view of Daphne, a \emph{task} is a \emph{operator} and some data.
An \emph{operator} is a single and indivisible operation that can be applied to the data of the task.
For example, in the case of a matrix-vector multiplication, the \emph{operator} the dot product, and the data will be a given number of rows of matrix, and the vector.
The number of rows of the matrix corresponds to the size of the \emph{task}.
In Daphne, a \emph{task} is the smallest unit of work, and cannot be subdivided.

Note that Daphne does not parallelize loop iteration like OpenMP, but instead parallelize and vectorize operations on matrices.

\subsection{Local runtime}\label{sec:daphnesched:local}

In this section, we summarize the content of \cite{eleliemy2023daphnesched}.

\qg{todo}

\paragraph{Scheduling schemes}

\ds\ supports 12 state-of-the-art scheduling, or partitioning, schemes, \ie\ how the total work (operation + data) is partitionned into several \emph{tasks}.

\paragraph{Worker}

TODO

\paragraph{Queue layouts}

After the work has been partitionned into \emph{tasks}, these \emph{tasks} are inserted into a \emph{queue}.
\ds\ has three queue layouts:

\begin{itemize}
\item a centralized queue (\texttt{CENTRALIZED}): all the worker query this one queue to get their next task
\item a per-socket queue (\texttt{PERGROUP}): a queue is created per socket, and the workers query the queue of the socket they belong. 
\item a per-cpu queue (\texttt{PERCPU}): each (allocated) CPU has one task queue, and all the workers running on this CPU query this queue.
\end{itemize}

\paragraph{Victim selections}

Having several queues (\ie\ \texttt{PERGROUP} and \texttt{PERCPU}) enables to apply work-stealing techniques.
Indeed once a of the queues is empty, all the worker belonging to this queue are now idle and looking for work.
\ds\ provides four victim selections:

\begin{itemize}
\item sequential stealing (\texttt{SEQ}): query the next queue in order for work
\item random stealing (\texttt{RANDOM}): query a random queue for work
\item sequential \emph{prioritized} stealing (\texttt{SEQPRI}): query the next queue \emph{in the same NUMA domain} for work
\item random \emph{prioritized} stealing (\texttt{RANDOMPRI}): query a random queue \emph{in the same NUMA domain} for work
\end{itemize}

Queue layout and victim selection play a crucial role in the performance of a IDA pipelines.
In some cases, a \texttt{CENTRALIZED} queue can introduce overhead because of lock contention, while \texttt{PERCPU} queues might induce a lot of steals which could increase the cache misses.

\qg{some image of the local rutime}

\subsection{Distributed runtime}

What was presented in Section \ref{sec:daphnesched:local} and in \cite{eleliemy2023daphnesched} is only for a single machine.
However, the ambition of Daphne is to be a \emph{scalable} runtime for IDA pipelines.
Hence, this section present the extension of \ds\ to support distributed executions.

The distributed runtime of Daphne bases itself on its local runtime.
There are two modes of the distributed runtime: using MPI, or using gRPC.
In this paper, we will focus on the MPI runtime; \qg{the gRPC will be presented.....}.

\ds\ leverages MPI to start multiple instances of the local runtime.
Rank 0 is called the \emph{coordinator}.
The \emph{coordinator} serves as an entry point and is responsible to send the tasks (data + operation) to the local runtimes.
Note that the \emph{coordinator} does not perform any computation.
The advantage of using MPI is the collective operations (\eg\ broadcast, scatter) in the case of sending the same data, or pieces of the whole data, to all the local runtimes.


\qg{some image of the distributed rutime}


\section{Importance of Scheduling}

\qg{in this section we show how the different option of daphnesched impact the performance. should this be before the section on comparison with different languages ?
It can also be presented as a platform for studying scheduling.
}

\qg{here: the image of the gantt of STATIC, GSS, and MFSC from the review meeting (generated with R to be vectoriel)}

\subsection{Considered Benchmarks}

\qg{TODO: small descriptions with math equations if applicable, give use-cases of algos}

\paragraph{Connected Components}

\paragraph{Page Rank}

assigns an ``importance'' weight to each node of $G$ based on the number of edges pointing from and to.

\paragraph{k-core}

computes the subgraph of $G$ where all the nodes have at least a degree of $k$.
To do so, it iteratively removes the nodes of unsufficient degree.

\paragraph{Linear Regression}

\paragraph{k-means}

\paragraph{N-body Simulation}

simulates the gravitational interaction between $N$ bodies.

\subsection{Considered Matrices}

The matrices have been optained on the \textit{SuiteSparse Matrix Collection} \cite{davis2011university}. 

\paragraph{\texttt{webbase-2001} \cite{BoVWFI, BRSLLP}}
which has been obtained from a 2001 crawl performed by the WebBase crawler.
The matrix represents the connections between 118 142 155 websites.


\paragraph{\qg{something from reto bsc thesis?}}

\subsection{Experiments}

\qg{on VEGA?, sciCORE?, miniHPC?}

\section{Comparison with popular scientific languages}

\begin{itemize}
    \item implement algorithms in an \textbf{idiomatic} way
    \item C++, Julia, Python, and DaphneDSL
\end{itemize}

\subsection{Comparison}

\begin{itemize}
\item number of lines
\item number of third party dependencies
\item distance between code and math equations
\item ``difficulty'' of implementation
\item \qg{disk size of binary + deps ?}
\end{itemize}

\paragraph{Third party dependencies}

One drawback of using general purpose programming languages for scientific computing is the need for third party dependencies.
These dependencies add complexity to the software environment \qg{more + repro?}.

The C++ implementations of the considered benchmarks depend on the Eigen library
Eigen \cite{guennebaud2010eigen} is a C++ library for linear algebra.
It supports operations on dense and spare matrices.
Eigen is parallelized via OpenMP.
Users can disable this parallelization at compile time, or set the number of threads via the \texttt{OMP\_NUM\_THREADS} environment variable.

The Python implementations depend on Numpy and Scipy.
Numpy \cite{harris2020array} and SciPy \cite{virtanen2020scipy} are the corner stone of scientific computing in Python.
The later depends on the former which is also parallelized via OpenMP.
Similarly to Eigen, users can set up \texttt{OMP\_NUM\_THREADS} to increase the number of threads.

The Julia implementations only required one external library to load the matrices of the MatrixMarket format.
All the computations use the standard Julia library. 

No external dependencies were needed for the DaphneDSL implementations.




\subsection{Experiments}

\qg{on VEGA?, sciCORE?, miniHPC?}

\section{Conlusion and Perspectives}

\section*{Acknowledgments}

This research was funded, in whole or in part, by the European Union’s Horizon 2020 research and innovation programme under grant agreement No. 957407 as DAPHNE.
The authors have applied a CC-BY public copyright license to the present document and will be applied to all subsequent versions up to the Author Accepted Manuscript arising from this submission, in accordance with the grant’s open access conditions.

\newpage

\printbibliography

\end{document}
